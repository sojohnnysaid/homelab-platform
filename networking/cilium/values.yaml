# Cilium Production Configuration
# Cluster: macmini-cluster (Talos Linux)
# Features: eBPF kube-proxy replacement, Hubble observability, WireGuard encryption

# ============================================================================
# CORE SETTINGS
# ============================================================================

# Replace kube-proxy with eBPF (strict mode)
kubeProxyReplacement: true

# Kubernetes API server endpoint (required for kube-proxy replacement)
k8sServiceHost: "192.168.1.223"
k8sServicePort: 6443

# Use native routing (host-gw equivalent, no encapsulation overhead)
routingMode: native
ipv4NativeRoutingCIDR: "10.244.0.0/16"
# Auto-create routes to other nodes' pod CIDRs
autoDirectNodeRoutes: true

# Auto-detect the best datapath mode
datapathMode: veth

# Enable IPv4, disable IPv6
ipam:
  mode: kubernetes
  operator:
    clusterPoolIPv4PodCIDRList:
      - "10.244.0.0/16"

# ============================================================================
# SECURITY - WIREGUARD ENCRYPTION
# ============================================================================

encryption:
  enabled: true
  type: wireguard
  wireguard:
    # Use userspace WireGuard if kernel module unavailable
    userspaceFallback: true

# ============================================================================
# OBSERVABILITY - HUBBLE
# ============================================================================

hubble:
  enabled: true

  # TLS configuration - use cronJob to generate certs at runtime (not at template time)
  # This prevents private keys from being embedded in manifests
  tls:
    auto:
      enabled: true
      method: cronJob
      certValidityDuration: 1095  # 3 years
      schedule: "0 0 1 */4 *"     # Rotate every 4 months

  # Hubble metrics for Prometheus
  metrics:
    enabled:
      - dns
      - drop
      - tcp
      - flow
      - icmp
      - http
    serviceMonitor:
      enabled: false  # Enable if using Prometheus Operator

  # Hubble Relay (aggregates flows from all nodes)
  relay:
    enabled: true
    replicas: 2
    tolerations:
      - operator: Exists
    resources:
      requests:
        cpu: 100m
        memory: 256Mi
      limits:
        cpu: 500m
        memory: 512Mi
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  k8s-app: hubble-relay
              topologyKey: kubernetes.io/hostname

  # Hubble UI (web dashboard for flow visualization)
  ui:
    enabled: true
    replicas: 2
    tolerations:
      - operator: Exists
    resources:
      requests:
        cpu: 50m
        memory: 128Mi
      limits:
        cpu: 200m
        memory: 256Mi
    frontend:
      resources:
        requests:
          cpu: 50m
          memory: 64Mi
        limits:
          cpu: 200m
          memory: 128Mi
    backend:
      resources:
        requests:
          cpu: 50m
          memory: 64Mi
        limits:
          cpu: 200m
          memory: 128Mi
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  k8s-app: hubble-ui
              topologyKey: kubernetes.io/hostname

# ============================================================================
# CILIUM OPERATOR
# ============================================================================

operator:
  replicas: 2
  resources:
    requests:
      cpu: 100m
      memory: 256Mi
    limits:
      cpu: 500m
      memory: 512Mi
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            labelSelector:
              matchLabels:
                io.cilium/app: operator
            topologyKey: kubernetes.io/hostname

# ============================================================================
# CILIUM AGENT (DaemonSet on each node)
# ============================================================================

resources:
  requests:
    cpu: 200m
    memory: 512Mi
  limits:
    cpu: 1000m
    memory: 1Gi

# BPF settings for performance
bpf:
  # Preallocate BPF maps for better performance
  preallocateMaps: true
  # Enable masquerading via BPF
  masquerade: true
  # Enable host routing for better performance
  hostLegacyRouting: false
  # Increase map sizes for production workloads
  mapDynamicSizeRatio: 0.0025
  # LB algorithm: maglev for consistent hashing
  lbAlgorithm: maglev
  # External traffic policy (local preserves source IP)
  lbExternalClusterIP: true

# ============================================================================
# CONTROL PLANE TOLERATIONS
# ============================================================================

# Allow Cilium to run on control plane nodes (required for your 3-node cluster)
tolerations:
  - operator: Exists

operator:
  tolerations:
    - operator: Exists

# ============================================================================
# ADDITIONAL FEATURES
# ============================================================================

# Enable local redirect policy for node-local DNS
localRedirectPolicy: true

# Enable bandwidth manager for better QoS
bandwidthManager:
  enabled: true
  bbr: true

# Enable health checking
healthChecking: true
healthPort: 9879

# Cluster name for multi-cluster setups
cluster:
  name: macmini-cluster
  id: 1

# Debug settings (disable in production after validation)
debug:
  enabled: false

# ============================================================================
# PROMETHEUS/MONITORING INTEGRATION
# ============================================================================

prometheus:
  enabled: true
  port: 9962
  serviceMonitor:
    enabled: false  # Enable if using Prometheus Operator

# Envoy proxy for L7 policies (optional, enable if needed)
envoy:
  enabled: false

# ============================================================================
# TALOS LINUX SPECIFIC
# ============================================================================

# Talos requires specific settings for Cilium
# See: https://docs.cilium.io/en/stable/installation/talos/

# Mount cgroup2 filesystem (required for Talos)
cgroup:
  autoMount:
    enabled: true
  hostRoot: /sys/fs/cgroup

# Talos uses cgroupv2

# Security context for Talos - requires privileged mode
securityContext:
  privileged: true
  allowPrivilegeEscalation: true
  capabilities:
    ciliumAgent:
      - CHOWN
      - KILL
      - NET_ADMIN
      - NET_RAW
      - IPC_LOCK
      - SYS_MODULE
      - SYS_ADMIN
      - SYS_RESOURCE
      - DAC_OVERRIDE
      - FOWNER
      - SETGID
      - SETUID
    cleanCiliumState:
      - NET_ADMIN
      - SYS_MODULE
      - SYS_ADMIN
      - SYS_RESOURCE

# BPF filesystem auto-mount
bpf:
  autoMount:
    enabled: true

# Required for kube-proxy replacement on Talos
# Talos doesn't run kube-proxy by default in some configs
socketLB:
  enabled: true

# Use host scope for IPAM since Talos manages node CIDRs
ipam:
  mode: kubernetes
