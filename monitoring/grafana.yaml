# Grafana with DNS and Log monitoring dashboards
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-datasources
  namespace: monitoring
data:
  datasources.yaml: |
    apiVersion: 1
    datasources:
    - name: Loki
      type: loki
      access: proxy
      url: http://loki:3100
      isDefault: true
      editable: false
    - name: Prometheus
      type: prometheus
      access: proxy
      url: http://prometheus:9090
      editable: false
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-dashboards-config
  namespace: monitoring
data:
  dashboards.yaml: |
    apiVersion: 1
    providers:
    - name: 'default'
      orgId: 1
      folder: ''
      type: file
      disableDeletion: true
      updateIntervalSeconds: 10
      allowUiUpdates: false
      options:
        path: /var/lib/grafana/dashboards
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-dashboards
  namespace: monitoring
data:
  dns-cache-dashboard.json: |
    {
      "title": "DNS Cache & Resolution Monitor",
      "uid": "dns-cache",
      "panels": [
        {
          "title": "DNS Query Rate",
          "gridPos": {"x": 0, "y": 0, "w": 12, "h": 8},
          "datasource": "Prometheus",
          "targets": [
            {
              "expr": "rate(coredns_dns_requests_total[5m])"
            }
          ],
          "type": "graph"
        },
        {
          "title": "DNS Cache Hit Ratio",
          "gridPos": {"x": 12, "y": 0, "w": 12, "h": 8},
          "datasource": "Prometheus",
          "targets": [
            {
              "expr": "coredns_cache_hits_total / (coredns_cache_hits_total + coredns_cache_misses_total)"
            }
          ],
          "type": "stat"
        },
        {
          "title": "DNS Errors (Last Hour)",
          "gridPos": {"x": 0, "y": 8, "w": 24, "h": 8},
          "datasource": "Loki",
          "targets": [
            {
              "expr": "{namespace=\"kube-system\", app=\"node-local-dns\"} |~ \"error|timeout|fail\""
            }
          ],
          "type": "logs"
        },
        {
          "title": "CloudFlare Tunnel Status",
          "gridPos": {"x": 0, "y": 16, "w": 24, "h": 8},
          "datasource": "Loki",
          "targets": [
            {
              "expr": "{app=\"cloudflared\"} |~ \"error|fail|disconnect|timeout\""
            }
          ],
          "type": "logs"
        }
      ],
      "refresh": "10s",
      "time": {"from": "now-1h", "to": "now"},
      "schemaVersion": 39
    }
  subdomain-availability-dashboard.json: |
    {
      "id": null,
      "uid": "subdomain-availability",
      "title": "Subdomain Availability",
      "tags": ["availability", "cloudflare", "subdomains"],
      "timezone": "browser",
      "schemaVersion": 39,
      "version": 1,
      "refresh": "30s",
      "time": {"from": "now-1h", "to": "now"},
      "panels": [
        {
          "id": 1,
          "title": "Current Status",
          "type": "stat",
          "gridPos": {"x": 0, "y": 0, "w": 24, "h": 4},
          "datasource": "Prometheus",
          "fieldConfig": {
            "defaults": {
              "mappings": [
                {"type": "value", "options": {"0": {"text": "DOWN", "color": "red"}}},
                {"type": "value", "options": {"1": {"text": "UP", "color": "green"}}}
              ],
              "thresholds": {
                "mode": "absolute",
                "steps": [
                  {"color": "red", "value": null},
                  {"color": "green", "value": 1}
                ]
              },
              "unit": "none"
            }
          },
          "options": {
            "colorMode": "background",
            "graphMode": "none",
            "justifyMode": "center",
            "textMode": "auto",
            "reduceOptions": {"calcs": ["lastNotNull"], "fields": "", "values": false}
          },
          "targets": [
            {
              "expr": "probe_success{job=\"blackbox-subdomain-availability\"}",
              "legendFormat": "{{subdomain}}",
              "refId": "A"
            }
          ]
        },
        {
          "id": 2,
          "title": "Availability Over Time",
          "type": "timeseries",
          "gridPos": {"x": 0, "y": 4, "w": 24, "h": 8},
          "datasource": "Prometheus",
          "fieldConfig": {
            "defaults": {
              "custom": {
                "drawStyle": "line",
                "lineWidth": 2,
                "fillOpacity": 20,
                "pointSize": 5,
                "showPoints": "auto"
              },
              "thresholds": {
                "mode": "absolute",
                "steps": [
                  {"color": "red", "value": null},
                  {"color": "green", "value": 1}
                ]
              },
              "min": 0,
              "max": 1
            }
          },
          "options": {"legend": {"displayMode": "table", "placement": "right"}},
          "targets": [
            {
              "expr": "probe_success{job=\"blackbox-subdomain-availability\"}",
              "legendFormat": "{{subdomain}}",
              "refId": "A"
            }
          ]
        },
        {
          "id": 3,
          "title": "Response Time (seconds)",
          "type": "timeseries",
          "gridPos": {"x": 0, "y": 12, "w": 24, "h": 8},
          "datasource": "Prometheus",
          "fieldConfig": {
            "defaults": {
              "custom": {
                "drawStyle": "line",
                "lineWidth": 1,
                "fillOpacity": 10
              },
              "unit": "s",
              "thresholds": {
                "mode": "absolute",
                "steps": [
                  {"color": "green", "value": null},
                  {"color": "yellow", "value": 1},
                  {"color": "red", "value": 3}
                ]
              }
            }
          },
          "options": {"legend": {"displayMode": "table", "placement": "right"}},
          "targets": [
            {
              "expr": "probe_duration_seconds{job=\"blackbox-subdomain-availability\"}",
              "legendFormat": "{{subdomain}}",
              "refId": "A"
            }
          ]
        },
        {
          "id": 4,
          "title": "Uptime % (24h)",
          "type": "stat",
          "gridPos": {"x": 0, "y": 20, "w": 24, "h": 4},
          "datasource": "Prometheus",
          "fieldConfig": {
            "defaults": {
              "unit": "percentunit",
              "decimals": 2,
              "thresholds": {
                "mode": "absolute",
                "steps": [
                  {"color": "red", "value": null},
                  {"color": "yellow", "value": 0.95},
                  {"color": "green", "value": 0.99}
                ]
              }
            }
          },
          "options": {
            "colorMode": "value",
            "graphMode": "area",
            "justifyMode": "center",
            "reduceOptions": {"calcs": ["mean"], "fields": "", "values": false}
          },
          "targets": [
            {
              "expr": "avg_over_time(probe_success{job=\"blackbox-subdomain-availability\"}[24h])",
              "legendFormat": "{{subdomain}}",
              "refId": "A"
            }
          ]
        },
        {
          "id": 5,
          "title": "SSL Certificate Expiry (days)",
          "type": "stat",
          "gridPos": {"x": 0, "y": 24, "w": 24, "h": 4},
          "datasource": "Prometheus",
          "fieldConfig": {
            "defaults": {
              "unit": "d",
              "decimals": 0,
              "thresholds": {
                "mode": "absolute",
                "steps": [
                  {"color": "red", "value": null},
                  {"color": "yellow", "value": 14},
                  {"color": "green", "value": 30}
                ]
              }
            }
          },
          "options": {
            "colorMode": "value",
            "graphMode": "none",
            "justifyMode": "center",
            "reduceOptions": {"calcs": ["lastNotNull"], "fields": "", "values": false}
          },
          "targets": [
            {
              "expr": "(probe_ssl_earliest_cert_expiry{job=\"blackbox-subdomain-availability\"} - time()) / 86400",
              "legendFormat": "{{subdomain}}",
              "refId": "A"
            }
          ]
        }
      ]
    }
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: grafana
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: grafana
  template:
    metadata:
      labels:
        app: grafana
    spec:
      tolerations:
      - key: node-role.kubernetes.io/control-plane
        operator: Exists
        effect: NoSchedule
      securityContext:
        fsGroup: 472
        runAsNonRoot: true
        runAsUser: 472
      containers:
      - name: grafana
        image: grafana/grafana:10.2.3
        ports:
        - containerPort: 3000
          name: http
        env:
        - name: GF_SECURITY_ADMIN_USER
          valueFrom:
            secretKeyRef:
              name: grafana-admin
              key: admin-user
        - name: GF_SECURITY_ADMIN_PASSWORD
          valueFrom:
            secretKeyRef:
              name: grafana-admin
              key: admin-password
        - name: GF_INSTALL_PLUGINS
          value: ""
        - name: GF_LOG_LEVEL
          value: warn  # Reduce log verbosity
        - name: GF_DATABASE_TYPE
          value: sqlite3
        - name: GF_DATABASE_WAL
          value: "true"
        resources:
          requests:
            cpu: 200m
            memory: 512Mi
          limits:
            cpu: 1000m
            memory: 1Gi
        volumeMounts:
        - name: datasources
          mountPath: /etc/grafana/provisioning/datasources
        - name: dashboards-config
          mountPath: /etc/grafana/provisioning/dashboards
        - name: dashboards
          mountPath: /var/lib/grafana/dashboards
        - name: storage
          mountPath: /var/lib/grafana
        livenessProbe:
          httpGet:
            path: /api/health
            port: 3000
          initialDelaySeconds: 30
        readinessProbe:
          httpGet:
            path: /api/health
            port: 3000
          initialDelaySeconds: 30
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          readOnlyRootFilesystem: true
      volumes:
      - name: datasources
        configMap:
          name: grafana-datasources
      - name: dashboards-config
        configMap:
          name: grafana-dashboards-config
      - name: dashboards
        configMap:
          name: grafana-dashboards
      - name: storage
        persistentVolumeClaim:
          claimName: grafana-storage
---
apiVersion: v1
kind: Service
metadata:
  name: grafana
  namespace: monitoring
spec:
  selector:
    app: grafana
  ports:
  - port: 3000
    targetPort: 3000
---
# Prometheus for metrics collection (minimal config for DNS metrics)
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: monitoring
data:
  alert.rules.yml: |
    groups:
    - name: smoke_test
      interval: 10s
      rules:
      - alert: EmailSmokeTestV2
        expr: vector(1)
        for: 0s
        labels:
          severity: info
          cluster: homelab
        annotations:
          summary: "SMTP Email Smoke Test V2"
          description: "This is a test alert to verify Alertmanager can send emails via smtp.gmail.com:587. If you receive this, the alerting system is working correctly with hostname-based SMTP."

  failover.rules.yml: |
    groups:
    - name: failover_monitoring
      interval: 10s
      rules:
      # Cloudflare Tunnel States
      - alert: CloudflareTunnelHealthy
        expr: |
          (probe_success{probe_type="public_https"} == 1
          unless
          (count(up{job="cloudflared"} == 1) < 2))
          and on()
          (hour() >= 11 and hour() < 11.25)
        for: 30s
        labels:
          severity: info
          component: cloudflare-tunnel
          state: healthy
        annotations:
          summary: "Cloudflare Tunnel is Healthy"
          description: "Cloudflare Tunnel has {{ $value }} healthy connections and public HTTPS probe is successful. Primary ingress path is operational. Daily health check at 6:00 AM EST."

      - alert: CloudflareTunnelDegraded
        expr: |
          (
            (count(up{job="cloudflared"} == 1) < 2 and count(up{job="cloudflared"} == 1) >= 1)
            or
            (probe_success{probe_type="public_https"} == 1 and probe_duration_seconds{probe_type="public_https"} > 2)
          )
        for: 15s
        labels:
          severity: warning
          component: cloudflare-tunnel
          state: degraded
        annotations:
          summary: "Cloudflare Tunnel is Degraded"
          description: "Cloudflare Tunnel has reduced capacity ({{ $value }} healthy pods) or high latency. Performance may be impacted."

      - alert: CloudflareTunnelDown
        expr: |
          (count(up{job="cloudflared"} == 1) or vector(0)) < 1
          or
          probe_success{probe_type="public_https"} == 0
        for: 15s
        labels:
          severity: critical
          component: cloudflare-tunnel
          state: down
        annotations:
          summary: "Cloudflare Tunnel is DOWN"
          description: "Cloudflare Tunnel has been down for more than 15 seconds. All cloudflared pods are offline or public HTTPS probe is failing. Failover to VPS should be active."

      # WireGuard VPS Failover States
      - alert: WireGuardFailoverReachable
        expr: probe_success{probe_type="vps_wireguard"} == 1
        for: 30s
        labels:
          severity: info
          component: vps-failover
          state: reachable
        annotations:
          summary: "WireGuard VPS Failover is Reachable"
          description: "WireGuard tunnel and VPS nginx are responding successfully. Failover path is available."

      - alert: WireGuardFailoverDegraded
        expr: |
          probe_success{probe_type="vps_wireguard"} == 1
          and
          probe_duration_seconds{probe_type="vps_wireguard"} > 1
        for: 15s
        labels:
          severity: warning
          component: vps-failover
          state: degraded
        annotations:
          summary: "WireGuard VPS Failover is Degraded"
          description: "VPS failover path has high latency ({{ $value }}s). Response time exceeds normal thresholds."

      - alert: WireGuardFailoverUnreachable
        expr: probe_success{probe_type="vps_wireguard"} == 0
        for: 15s
        labels:
          severity: critical
          component: vps-failover
          state: unreachable
        annotations:
          summary: "WireGuard VPS Failover is UNREACHABLE"
          description: "WireGuard tunnel or VPS nginx is not responding. Failover path is unavailable. Check WireGuard tunnel and VPS health."

      # Dual Failure State
      - alert: DualFailure
        expr: |
          (probe_success{probe_type="public_https"} == 0 or count(up{job="cloudflared"} == 1) == 0)
          and on()
          probe_success{probe_type="vps_wireguard"} == 0
        for: 15s
        labels:
          severity: critical
          component: all-paths
          state: dual_failure
        annotations:
          summary: "DUAL FAILURE - All Ingress Paths Down"
          description: "CRITICAL: Both Cloudflare Tunnel AND VPS failover are unreachable. mirai.sogos.io is completely offline. Immediate action required."

      # Recovery Events
      - alert: CloudflareTunnelRecovered
        expr: |
          probe_success{probe_type="public_https"} == 1
          unless
          (count(up{job="cloudflared"} == 1) < 2)
          unless
          (count(up{job="cloudflared"} offset 5m == 1) >= 2 and probe_success{probe_type="public_https"} offset 5m == 1)
        for: 30s
        labels:
          severity: info
          component: cloudflare-tunnel
          state: recovered
        annotations:
          summary: "Cloudflare Tunnel has RECOVERED"
          description: "Cloudflare Tunnel is now healthy after previous failure. Cloudflared pods are running and public HTTPS probe is successful."

      - alert: TrafficSwitchBackToCloudflare
        expr: |
          probe_success{probe_type="public_https"} == 1
          unless
          (count(up{job="cloudflared"} == 1) < 2)
          unless
          (probe_success{probe_type="vps_wireguard"} == 0)
          unless
          (count(up{job="cloudflared"} offset 10m == 1) >= 2 and probe_success{probe_type="public_https"} offset 10m == 1)
        for: 2m
        labels:
          severity: info
          component: traffic-routing
          state: switch_back
        annotations:
          summary: "Traffic Switched Back to Cloudflare Tunnel"
          description: "Cloudflare Tunnel has recovered and is stable. Traffic routing should now prefer Cloudflare Tunnel over VPS failover. Both paths are healthy."

  prometheus.yml: |
    global:
      scrape_interval: 15s
      evaluation_interval: 15s
      external_labels:
        cluster: 'macmini-cluster'

    alerting:
      alertmanagers:
      - static_configs:
        - targets:
          - 10.97.174.225:9093

    rule_files:
    - '/etc/prometheus/alert.rules.yml'
    - '/etc/prometheus/failover.rules.yml'

    scrape_configs:
    # DNS metrics
    - job_name: 'coredns'
      kubernetes_sd_configs:
      - role: pod
        namespaces:
          names:
          - kube-system
      relabel_configs:
      - source_labels: [__meta_kubernetes_pod_name]
        action: keep
        regex: coredns.*
      - source_labels: [__address__]
        action: replace
        regex: ([^:]+)(?::\d+)?
        replacement: $1:9153
        target_label: __address__

    - job_name: 'node-local-dns'
      kubernetes_sd_configs:
      - role: pod
        namespaces:
          names:
          - kube-system
      relabel_configs:
      - source_labels: [__meta_kubernetes_pod_label_k8s_app]
        action: keep
        regex: node-local-dns
      - source_labels: [__address__]
        action: replace
        regex: ([^:]+)(?::\d+)?
        replacement: $1:9253
        target_label: __address__

    # Kubernetes API Server metrics
    - job_name: 'kubernetes-apiservers'
      kubernetes_sd_configs:
      - role: endpoints
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      relabel_configs:
      - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
        action: keep
        regex: default;kubernetes;https

    # Kubelet metrics (container/pod metrics via cAdvisor)
    - job_name: 'kubernetes-nodes'
      kubernetes_sd_configs:
      - role: node
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        insecure_skip_verify: true
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)

    # Kubelet cAdvisor metrics (detailed container metrics)
    - job_name: 'kubernetes-cadvisor'
      kubernetes_sd_configs:
      - role: node
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        insecure_skip_verify: true
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)
      - target_label: __metrics_path__
        replacement: /metrics/cadvisor

    # Node Exporter (hardware and OS metrics)
    - job_name: 'node-exporter'
      kubernetes_sd_configs:
      - role: pod
        namespaces:
          names:
          - monitoring
      relabel_configs:
      - source_labels: [__meta_kubernetes_pod_label_app]
        action: keep
        regex: node-exporter
      - source_labels: [__meta_kubernetes_pod_node_name]
        target_label: node
      - source_labels: [__address__]
        action: replace
        regex: ([^:]+)(?::\d+)?
        replacement: $1:9100
        target_label: __address__

    # Pod metrics across all namespaces
    - job_name: 'kubernetes-pods'
      kubernetes_sd_configs:
      - role: pod
      relabel_configs:
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
        action: keep
        regex: true
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
        action: replace
        target_label: __metrics_path__
        regex: (.+)
      - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
        action: replace
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
        target_label: __address__
      - action: labelmap
        regex: __meta_kubernetes_pod_label_(.+)
      - source_labels: [__meta_kubernetes_namespace]
        target_label: namespace
      - source_labels: [__meta_kubernetes_pod_name]
        target_label: pod

    # etcd metrics (Talos exposes on control plane nodes)
    - job_name: 'etcd'
      kubernetes_sd_configs:
      - role: node
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        insecure_skip_verify: true
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      relabel_configs:
      - source_labels: [__meta_kubernetes_node_label_node_role_kubernetes_io_control_plane]
        action: keep
        regex: "true"
      - source_labels: [__address__]
        action: replace
        regex: ([^:]+)(?::\d+)?
        replacement: $1:2381
        target_label: __address__
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)

    # kube-controller-manager metrics
    - job_name: 'kube-controller-manager'
      kubernetes_sd_configs:
      - role: node
      scheme: https
      tls_config:
        insecure_skip_verify: true
      relabel_configs:
      - source_labels: [__meta_kubernetes_node_label_node_role_kubernetes_io_control_plane]
        action: keep
        regex: "true"
      - source_labels: [__address__]
        action: replace
        regex: ([^:]+)(?::\d+)?
        replacement: $1:10257
        target_label: __address__
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)

    # kube-scheduler metrics
    - job_name: 'kube-scheduler'
      kubernetes_sd_configs:
      - role: node
      scheme: https
      tls_config:
        insecure_skip_verify: true
      relabel_configs:
      - source_labels: [__meta_kubernetes_node_label_node_role_kubernetes_io_control_plane]
        action: keep
        regex: "true"
      - source_labels: [__address__]
        action: replace
        regex: ([^:]+)(?::\d+)?
        replacement: $1:10259
        target_label: __address__
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)

    # Cloudflared tunnel metrics
    - job_name: 'cloudflared'
      kubernetes_sd_configs:
      - role: pod
        namespaces:
          names:
          - ingress
      relabel_configs:
      - source_labels: [__meta_kubernetes_pod_label_app]
        action: keep
        regex: cloudflared
      - source_labels: [__address__]
        action: replace
        regex: ([^:]+)(?::\d+)?
        replacement: $1:2000
        target_label: __address__
      - source_labels: [__meta_kubernetes_pod_name]
        target_label: pod
      - target_label: component
        replacement: cloudflare-tunnel

    # Blackbox exporter probes for failover monitoring
    - job_name: 'blackbox-public-https'
      metrics_path: /probe
      params:
        module: [http_2xx]
      static_configs:
      - targets:
        - https://mirai.sogos.io
      relabel_configs:
      - source_labels: [__address__]
        target_label: __param_target
      - source_labels: [__param_target]
        target_label: instance
      - target_label: __address__
        replacement: blackbox-exporter.monitoring.svc.cluster.local:9115
      - target_label: probe_type
        replacement: public_https
      - target_label: component
        replacement: cloudflare-edge

    - job_name: 'blackbox-vps-wireguard'
      metrics_path: /probe
      params:
        module: [http_2xx]
      static_configs:
      - targets:
        - http://wireguard-failover.ingress.svc.cluster.local:80
      relabel_configs:
      - source_labels: [__address__]
        target_label: __param_target
      - source_labels: [__param_target]
        target_label: instance
      - target_label: __address__
        replacement: blackbox-exporter.monitoring.svc.cluster.local:9115
      - target_label: probe_type
        replacement: vps_wireguard
      - target_label: component
        replacement: vps-failover

    # Subdomain availability monitoring (separate from failover probes)
    - job_name: 'blackbox-subdomain-availability'
      metrics_path: /probe
      params:
        module: [http_2xx]
      static_configs:
      - targets:
        - https://health.sogos.io
        - https://mirai.sogos.io
        - https://get-mirai.sogos.io
        - https://mirai-api.sogos.io
        - https://mirai-auth.sogos.io
        - https://mailpit.sogos.io
        - https://argocd.sogos.io
        - https://grafana.sogos.io
        - https://hello.sogos.io
        - https://solutions.sogos.io
        - https://product.sogos.io
      relabel_configs:
      - source_labels: [__address__]
        target_label: __param_target
      - source_labels: [__param_target]
        target_label: instance
      - source_labels: [__param_target]
        regex: 'https://([^.]+)\.sogos\.io'
        replacement: '$1'
        target_label: subdomain
      - target_label: __address__
        replacement: blackbox-exporter.monitoring.svc.cluster.local:9115
      - target_label: probe_type
        replacement: subdomain_availability
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: prometheus
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: prometheus
  template:
    metadata:
      labels:
        app: prometheus
    spec:
      tolerations:
      - key: node-role.kubernetes.io/control-plane
        operator: Exists
        effect: NoSchedule
      serviceAccountName: prometheus
      securityContext:
        fsGroup: 65534
        runAsNonRoot: true
        runAsUser: 65534
      containers:
      - name: prometheus
        image: prom/prometheus:v2.48.1
        args:
        - '--config.file=/etc/prometheus/prometheus.yml'
        - '--storage.tsdb.path=/prometheus'
        - '--storage.tsdb.retention.time=7d'  # Keep metrics for 7 days only
        - '--storage.tsdb.retention.size=5GB'  # Max 5GB storage
        - '--web.enable-lifecycle'
        ports:
        - containerPort: 9090
          name: http
        resources:
          requests:
            cpu: 500m
            memory: 2Gi
          limits:
            cpu: 2000m
            memory: 8Gi
        volumeMounts:
        - name: config
          mountPath: /etc/prometheus
        - name: storage
          mountPath: /prometheus
        livenessProbe:
          httpGet:
            path: /-/healthy
            port: 9090
          initialDelaySeconds: 30
        readinessProbe:
          httpGet:
            path: /-/ready
            port: 9090
          initialDelaySeconds: 30
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          readOnlyRootFilesystem: true
      volumes:
      - name: config
        configMap:
          name: prometheus-config
      - name: storage
        persistentVolumeClaim:
          claimName: prometheus-storage
---
apiVersion: v1
kind: Service
metadata:
  name: prometheus
  namespace: monitoring
spec:
  selector:
    app: prometheus
  ports:
  - port: 9090
    targetPort: 9090
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: prometheus
  namespace: monitoring
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: prometheus
rules:
- apiGroups: [""]
  resources:
  - nodes
  - nodes/proxy
  - services
  - endpoints
  - pods
  verbs: ["get", "list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: prometheus
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: prometheus
subjects:
- kind: ServiceAccount
  name: prometheus
  namespace: monitoring